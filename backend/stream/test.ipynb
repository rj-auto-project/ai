{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  PIL import Image\n",
    "import PIL.Image\n",
    "import redis\n",
    "import PIL\n",
    "import os\n",
    "import cv2\n",
    "# from module import k_mean_color_detection\n",
    "import numpy as np\n",
    "import random\n",
    "from module import  process_raw_d_logs, process_raw_cc_logs, process_cc_logs\n",
    "\n",
    "# Connect to the Redis server\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "def clear_redis_database():\n",
    "    r.flushdb()\n",
    "    print(\"Redis database cleared.\")\n",
    "\n",
    "# Function to store string data with key namespacing\n",
    "def store_string_with_namespace(serial_number, constant_string, value):\n",
    "    full_key = f\"{serial_number}:{constant_string}\"\n",
    "    r.set(full_key, value)\n",
    "    print(f\"Stored {full_key} -> {value}\")\n",
    "\n",
    "# Function to retrieve all data matching the constant part of the key\n",
    "def retrieve_keys(constant_string):\n",
    "    matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "    results = {}\n",
    "    for key in matching_keys:\n",
    "        value = r.get(key)\n",
    "        if value:\n",
    "            decoded_key = key.decode()\n",
    "            decoded_value = value.decode()\n",
    "            # np_arr = np.frombuffer(value, np.uint8)\n",
    "            # Decode the numpy array back to an image\n",
    "            # retrieved_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "            # results[decoded_key] = retrieved_image\n",
    "            results[decoded_key] = decoded_value\n",
    "            # r.delete(decoded_key)\n",
    "            # print(results)\n",
    "            # r.delete(key)  # Delete the key after retrieving the value\n",
    "            # print(f\"Retrieved and deleted {decoded_key} -> {decoded_value}\")\n",
    "    return results\n",
    "\n",
    "def retrieve_image(constant_string):\n",
    "    matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "    results = {}\n",
    "    for key in matching_keys:\n",
    "        value = r.get(key)\n",
    "        if value:\n",
    "            decoded_key = key.decode()\n",
    "            # decoded_value = value.decode()\n",
    "            np_arr = np.frombuffer(value, np.uint8)\n",
    "            # Decode the numpy array back to an image\n",
    "            image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "            # image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "            # image = image.reshape((-1, image.shape[2]))\n",
    "            # cv2.imwrite(f\"/home/annone/ai/backend/stream/d_temp/{decoded_key}.png\", image)\n",
    "            results[decoded_key] = image\n",
    "            # results[decoded_key] = decoded_value\n",
    "            # r.delete(decoded_key)\n",
    "            # print(results)\n",
    "            # r.delete(key)  # Delete the key after retrieving the value\n",
    "            # print(f\"Retrieved and deleted {decoded_key} -> {decoded_value}\")\n",
    "    return results\n",
    "\n",
    "# def process_raw_d_logs(constant_string):\n",
    "#     while True:\n",
    "#         matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "#         # print(matching_keys)\n",
    "#         for key in matching_keys:\n",
    "#             value = r.get(key)\n",
    "#             decode_key = key.decode()\n",
    "#             decode_value = value.decode().split(\"|\")\n",
    "#             track_id = decode_value[5]\n",
    "#             image_path = decode_value[7]\n",
    "#             try:\n",
    "#                 image = cv2.imread(f\"/home/annone/ai-camera/backend/stream/temp/{image_path}\")\n",
    "#                 image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "#                 aa = k_mean_color_detection(image)\n",
    "#                 decode_value[7]  = aa\n",
    "#                 r.set(f\"{track_id}_{random.randint(0,9999)}:d_log\",\"|\".join(decode_value))\n",
    "#                 os.remove(f\"/home/annone/ai-camera/backend/stream/temp/{image_path}\")\n",
    "#                 r.delete(decode_key)\n",
    "#             except:\n",
    "#                 # print(\"excepted\")\n",
    "#                 continue\n",
    "            # show_rgb_colors(aa,image)\n",
    "            # print(decode_value)\n",
    "\n",
    "# def process_d_logs(constant_string):\n",
    "#     conn = Database.get_connection()\n",
    "#     cursor = conn.cursor()\n",
    "#     batch = []\n",
    "#     while True:\n",
    "#         matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "#         for key in matching_keys:\n",
    "#             value = r.get(key)\n",
    "#             decode_key = key.decode()\n",
    "#             decode_value = value.decode().split(\"|\")\n",
    "#             batch.append(tuple(decode_value))\n",
    "#         if len(batch) > 0:\n",
    "#             query = 'INSERT INTO \"DetectionLog\" (\"cameraId\", \"camera_ip\", \"timestamp\", \"boxCoords\", \"detectionClass\", \"trackId\", \"classConfidence\", \"metadata\") VALUES %s;'\n",
    "#             execute_values(cursor, query, batch)\n",
    "#             conn.commit()\n",
    "#             cursor.close()\n",
    "#             conn.close()\n",
    "#             batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_image(\"image\")[\"817_qcYXs5PM_39_1724913805_127_0_0_1_976:image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_keys(\"raw_d_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_redis_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Get all keys from Redis\n",
    "keys = r.keys('*')\n",
    "\n",
    "# Loop through the keys and get the data for each key\n",
    "for i, key in enumerate(keys):\n",
    "    if i ==10:\n",
    "        break\n",
    "    # Decode key to get human-readable format\n",
    "    decoded_key = key.decode('utf-8')\n",
    "    \n",
    "    # Get the value associated with the key\n",
    "    value = r.get(key)\n",
    "    \n",
    "    # Decode the value, if it's stored as a string\n",
    "    decoded_value = value.decode('utf-8') if value else None\n",
    "    \n",
    "    # Print the key-value pair\n",
    "    print(f\"Key: {decoded_key}, Value: {decoded_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundig Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from sort.sort import Sort  # Example of an external tracking library\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLO(\"/home/annone/ai/models/objseg50e.pt\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "tracker = Sort()\n",
    "\n",
    "# Function to process frames in batches\n",
    "def process_frame_batch(frames):\n",
    "    resized_frames = [cv2.resize(frame, (640 // 32 * 32, 480 // 32 * 32)) for frame in frames]\n",
    "\n",
    "    frames_tensor = torch.from_numpy(np.stack(resized_frames)).permute(0, 3, 1, 2).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_results = model(frames_tensor, device=device)\n",
    "\n",
    "    return batch_results, resized_frames\n",
    "\n",
    "def track_objects(frames, batch_results):\n",
    "    tracked_frames = []\n",
    "    \n",
    "    for frame, result in zip(frames, batch_results):\n",
    "        detections = []\n",
    "        \n",
    "        if result.boxes:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                score = box.conf[0].item()\n",
    "                label = model.names[int(box.cls[0])]\n",
    "                \n",
    "                detections.append([x1, y1, x2, y2, score])\n",
    "\n",
    "        tracks = tracker.update(np.array(detections))\n",
    "\n",
    "        for track in tracks:\n",
    "            track_id = int(track[4])\n",
    "            x1, y1, x2, y2 = map(int, track[:4])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        tracked_frames.append(frame)\n",
    "    \n",
    "    return tracked_frames\n",
    "\n",
    "def stream_process(camera_id, camera_ip, video_path, batch_size=8):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    t1 = time.time()\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "        if len(frames) >= batch_size:\n",
    "            batch_results, resized_frames = process_frame_batch(frames)\n",
    "\n",
    "            tracked_frames = track_objects(resized_frames, batch_results)\n",
    "\n",
    "            for tracked_frame in tracked_frames:\n",
    "                cv2.imshow(\"Tracked Frame\", tracked_frame)\n",
    "            frames = []\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "video_path = '/home/annone/ai/data/T-pole wrong way.mp4'\n",
    "cam_ip = '127.0.0.1'\n",
    "cam_id = \"1\"\n",
    "stream_process(cam_id, cam_ip, video_path, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=10\n",
    "count = 0\n",
    "total_parking_violations = 0 \n",
    "wrong_way_violation_count = 0\n",
    "traffic_violation_count = 0  \n",
    "crossed_objects = {}  # Track objects that have crossed lines\n",
    "violated_objects = set()  # Track objects that have already violated\n",
    "static_objects = {}  # To track objects that are stationary\n",
    "stationary_frame_threshold = 200\n",
    "# parking\n",
    "def check_illegal_parking(track_id, cx, cy):\n",
    "    \"\"\"Check if an object is illegally parked based on stationary duration.\"\"\"\n",
    "    global static_objects, total_parking_violations\n",
    "\n",
    "    # Initialize tracking information if not already present\n",
    "    if track_id not in static_objects:\n",
    "        static_objects[track_id] = {\"position\": (cx, cy), \"frames\": 0, \"violated\": False}\n",
    "    else:\n",
    "        last_position = static_objects[track_id][\"position\"]\n",
    "\n",
    "        # Check if the object has remained stationary (within a certain offset)\n",
    "        if abs(cx - last_position[0]) <= offset and abs(cy - last_position[1]) <= offset:\n",
    "            static_objects[track_id][\"frames\"] += 1\n",
    "            # If the object is stationary for more than the threshold and not already marked as a violation\n",
    "            if static_objects[track_id][\"frames\"] > stationary_frame_threshold and not static_objects[track_id][\"violated\"]:\n",
    "                static_objects[track_id][\"violated\"] = True  # Mark the object as a violation\n",
    "                total_parking_violations += 1  # Increase the parking violation count\n",
    "                print(f\"Object {track_id} marked as parking violation. Total Violations: {total_parking_violations}\")\n",
    "        else:\n",
    "            # If the object has moved, reset its tracking information\n",
    "            static_objects[track_id][\"position\"] = (cx, cy)\n",
    "            static_objects[track_id][\"frames\"] = 0\n",
    "            static_objects[track_id][\"violated\"] = False\n",
    "# wrong way\n",
    "def detect_wrong_way_violation(track_id, cx, cy):\n",
    "    global wrong_way_violation_count, violated_objects\n",
    "\n",
    "    # Initialize tracking for the object if not already done\n",
    "    if track_id not in crossed_objects:\n",
    "        crossed_objects[track_id] = {'red': set(), 'green': set()}\n",
    "\n",
    "    # Check if the object crosses any red line\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(red_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                crossed_objects[track_id]['red'].add(i)\n",
    "\n",
    "    # Check if the object crosses any green line\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(green_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                crossed_objects[track_id]['green'].add(i)\n",
    "\n",
    "    # Detect wrong-way violation (crossing green line after red line)\n",
    "    if any(\n",
    "        i in crossed_objects[track_id]['green'] and\n",
    "        i in crossed_objects[track_id]['red'] and\n",
    "        track_id not in violated_objects\n",
    "        for i in crossed_objects[track_id]['green']\n",
    "    ):\n",
    "        # cv2.putText(frame, \"Wrong Way Violation\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        wrong_way_violation_count += 1\n",
    "        violated_objects.add(track_id)\n",
    "# red light violation\n",
    "def detect_traffic_violation(track_id, cx, cy):\n",
    "    \"\"\"Check if an object violates traffic rules by crossing lines.\"\"\"\n",
    "    global traffic_violation_count, crossed_objects, violated_objects\n",
    "\n",
    "    # Initialize tracking for the object if not already done\n",
    "    if track_id not in crossed_objects:\n",
    "        crossed_objects[track_id] = set()\n",
    "\n",
    "    # Check if the object crosses any red line\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(red_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                crossed_objects[track_id].add(f\"red_{i}\")\n",
    "\n",
    "    # Check if the object crosses any green line after crossing a red line (indicating a violation)\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(green_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                if any(f\"red_{j}\" in crossed_objects[track_id] for j in range(len(red_lines))) and track_id not in violated_objects:\n",
    "                    # cv2.putText(frame, \"Traffic Violation\", (cx, cy - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "                    traffic_violation_count += 1\n",
    "                    violated_objects.add(track_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu as Computation Device initiated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.5-0ubuntu1~22.04.sav0 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.4.0-1ubuntu1~22.04)\n",
      "  configuration: --prefix=/usr --extra-version='0ubuntu1~22.04.sav0' --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 32.7ms\n",
      "1: 480x640 2 hatchbacks, 1 pickup, 2 scooty-riders, 32.7ms\n",
      "Speed: 0.0ms preprocess, 32.7ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 hatchbacks, 1 pickup, 2 scooty-riders, 36.6ms\n",
      "1: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 36.6ms\n",
      "Speed: 0.0ms preprocess, 36.6ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, rawvideo, from 'pipe:':\n",
      "  Duration: N/A, start: 0.000000, bitrate: 184320 kb/s\n",
      "  Stream #0:0: Video: rawvideo (BGR[24] / 0x18524742), bgr24, 640x480, 184320 kb/s, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (rawvideo (native) -> h264 (libx264))\n",
      "[libx264 @ 0x5bb3f0072940] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x5bb3f0072940] profile High 4:4:4 Predictive, level 3.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x5bb3f0072940] 264 - core 164 r3191 4613ac3 - H.264/MPEG-4 AVC codec - Copyleft 2003-2024 - http://www.videolan.org/x264.html - options: cabac=0 ref=1 deblock=0:0:0 analyse=0:0 me=dia subme=0 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=0 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=6 threads=7 lookahead_threads=7 sliced_threads=1 slices=7 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=25 scenecut=0 intra_refresh=0 rc=crf mbtree=0 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=0\n",
      "Output #0, rtsp, to 'rtsp://localhost:8554/stream1':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264, yuv444p(tv, progressive), 640x480, q=2-31, 25 fps, 90k tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=    1 fps=0.0 q=20.0 size=N/A time=00:00:00.00 bitrate=N/A speed=  11x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 37.9ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 37.9ms\n",
      "Speed: 0.0ms preprocess, 37.9ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 35.5ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 35.5ms\n",
      "Speed: 0.0ms preprocess, 35.5ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 34.7ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 34.7ms\n",
      "Speed: 0.0ms preprocess, 34.7ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 35.6ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 35.6ms\n",
      "Speed: 0.0ms preprocess, 35.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   27 fps=0.0 q=20.0 size=N/A time=00:00:01.04 bitrate=N/A speed=1.93x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 1 scooty-rider, 48.8ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 48.8ms\n",
      "Speed: 0.0ms preprocess, 48.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 36.2ms\n",
      "1: 480x640 1 pickup, 36.2ms\n",
      "Speed: 0.0ms preprocess, 36.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 33.7ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 33.7ms\n",
      "Speed: 0.0ms preprocess, 33.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 29.5ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 29.5ms\n",
      "Speed: 0.0ms preprocess, 29.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 30.1ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 30.1ms\n",
      "Speed: 0.0ms preprocess, 30.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   43 fps= 37 q=23.0 size=N/A time=00:00:01.68 bitrate=N/A speed=1.46x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 1 scooty-rider, 28.7ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 28.7ms\n",
      "Speed: 0.0ms preprocess, 28.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 28.7ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 28.7ms\n",
      "Speed: 0.0ms preprocess, 28.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 28.0ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 28.0ms\n",
      "Speed: 0.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 28.1ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 28.1ms\n",
      "Speed: 0.0ms preprocess, 28.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   58 fps= 35 q=20.0 size=N/A time=00:00:02.28 bitrate=N/A speed=1.36x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 1 scooty-rider, 28.7ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 28.7ms\n",
      "Speed: 0.0ms preprocess, 28.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 27.7ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 27.7ms\n",
      "Speed: 0.0ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 21.9ms\n",
      "1: 480x640 1 pickup, 21.9ms\n",
      "Speed: 0.0ms preprocess, 21.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 31.3ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 31.3ms\n",
      "Speed: 0.0ms preprocess, 31.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   74 fps= 34 q=19.0 size=N/A time=00:00:02.92 bitrate=N/A speed=1.33x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 32.8ms\n",
      "1: 480x640 1 pickup, 32.8ms\n",
      "Speed: 0.0ms preprocess, 32.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 31.1ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 31.1ms\n",
      "Speed: 0.0ms preprocess, 31.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 30.7ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 30.7ms\n",
      "Speed: 0.0ms preprocess, 30.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 30.7ms\n",
      "1: 480x640 1 pickup, 30.7ms\n",
      "Speed: 0.0ms preprocess, 30.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   83 fps= 30 q=20.0 size=N/A time=00:00:03.28 bitrate=N/A speed=1.19x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 1 scooty-rider, 31.5ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 31.5ms\n",
      "Speed: 0.0ms preprocess, 31.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 30.7ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 30.7ms\n",
      "Speed: 0.0ms preprocess, 30.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 31.7ms\n",
      "1: 480x640 1 pickup, 31.7ms\n",
      "Speed: 0.0ms preprocess, 31.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 32.9ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 32.9ms\n",
      "Speed: 0.0ms preprocess, 32.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   94 fps= 29 q=19.0 size=N/A time=00:00:03.72 bitrate=N/A speed=1.13x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 29.8ms\n",
      "1: 480x640 1 pickup, 29.8ms\n",
      "Speed: 0.0ms preprocess, 29.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 30.3ms\n",
      "1: 480x640 1 pickup, 30.3ms\n",
      "Speed: 0.0ms preprocess, 30.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 33.4ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 33.4ms\n",
      "Speed: 0.0ms preprocess, 33.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 31.2ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  103 fps= 27 q=22.0 size=N/A time=00:00:04.08 bitrate=N/A speed=1.07x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 2 scooty-riders, 30.9ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 30.9ms\n",
      "Speed: 0.0ms preprocess, 30.9ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 35.2ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 35.2ms\n",
      "Speed: 0.0ms preprocess, 35.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 38.0ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 38.0ms\n",
      "Speed: 0.0ms preprocess, 38.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 46.1ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 46.1ms\n",
      "Speed: 0.0ms preprocess, 46.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  118 fps= 27 q=20.0 size=N/A time=00:00:04.68 bitrate=N/A speed=1.07x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 2 scooty-riders, 36.8ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 36.8ms\n",
      "Speed: 0.0ms preprocess, 36.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 36.1ms\n",
      "1: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 36.1ms\n",
      "Speed: 0.0ms preprocess, 36.1ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 35.1ms\n",
      "1: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 35.1ms\n",
      "Speed: 0.0ms preprocess, 35.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 33.3ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 33.3ms\n",
      "Speed: 0.0ms preprocess, 33.3ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  139 fps= 28 q=19.0 size=N/A time=00:00:05.52 bitrate=N/A speed=1.12x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 37.8ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 37.8ms\n",
      "Speed: 0.0ms preprocess, 37.8ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 36.6ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 36.6ms\n",
      "Speed: 0.0ms preprocess, 36.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 33.9ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 33.9ms\n",
      "Speed: 0.0ms preprocess, 33.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 31.5ms\n",
      "1: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 31.5ms\n",
      "Speed: 0.0ms preprocess, 31.5ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  160 fps= 29 q=19.0 size=N/A time=00:00:06.36 bitrate=N/A speed=1.17x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 pickup, 2 scooty-riders, 34.3ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 34.3ms\n",
      "Speed: 0.0ms preprocess, 34.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 hatchback, 1 pickup, 2 scooty-riders, 32.7ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 32.7ms\n",
      "Speed: 0.0ms preprocess, 32.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 26.3ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 26.3ms\n",
      "Speed: 0.0ms preprocess, 26.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 31.3ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 31.3ms\n",
      "Speed: 0.0ms preprocess, 31.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  181 fps= 30 q=19.0 size=N/A time=00:00:07.20 bitrate=N/A speed=1.21x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 car, 1 pickup, 1 scooty-rider, 33.0ms\n",
      "1: 480x640 1 pickup, 1 scooty-rider, 33.0ms\n",
      "Speed: 0.0ms preprocess, 33.0ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 1 scooty-rider, 25.1ms\n",
      "1: 480x640 1 pickup, 2 scooty-riders, 25.1ms\n",
      "Speed: 0.0ms preprocess, 25.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 pickup, 2 scooty-riders, 24.5ms\n",
      "1: 480x640 1 scooty-rider, 24.5ms\n",
      "Speed: 0.0ms preprocess, 24.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 2 scooty-riders, 25.5ms\n",
      "1: 480x640 (no detections), 25.5ms\n",
      "Speed: 0.0ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  201 fps= 31 q=20.0 size=N/A time=00:00:08.00 bitrate=N/A speed=1.23x    \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 200\u001b[0m\n\u001b[1;32m    198\u001b[0m cam_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    199\u001b[0m cam_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mstream_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcam_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam_ip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 180\u001b[0m, in \u001b[0;36mstream_process\u001b[0;34m(camera_id, camera_ip, video_path, batch_size)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m    178\u001b[0m     batch_results, resized_frames \u001b[38;5;241m=\u001b[39m process_frame_batch(frames)\n\u001b[0;32m--> 180\u001b[0m     tracked_frames, track_id_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrack_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tracked_frame \u001b[38;5;129;01min\u001b[39;00m tracked_frames:\n\u001b[1;32m    182\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTracked Frame\u001b[39m\u001b[38;5;124m\"\u001b[39m, tracked_frame)\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mtrack_objects\u001b[0;34m(frames, batch_results, frame_time)\u001b[0m\n\u001b[1;32m    103\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mpolylines(frame, [np\u001b[38;5;241m.\u001b[39marray(mask, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)], isClosed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.6\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, track \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tracks):\n\u001b[1;32m    109\u001b[0m     track_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(track[\u001b[38;5;241m4\u001b[39m])\n",
      "File \u001b[0;32m~/ai/backend/stream/sort/sort.py:231\u001b[0m, in \u001b[0;36mSort.update\u001b[0;34m(self, dets)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(to_del):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mpop(t)\n\u001b[0;32m--> 231\u001b[0m matched, unmatched_dets, unmatched_trks \u001b[38;5;241m=\u001b[39m associate_detections_to_trackers(\u001b[43mdets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m, trks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou_threshold)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# update matched trackers with assigned detections\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matched:\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from sort.sort import Sort\n",
    "import time\n",
    "# from module import generate_custom_string\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import redis\n",
    "import uuid\n",
    "import subprocess\n",
    "\n",
    "# Initialize device and model\n",
    "PARENT_DIR = \"/home/annone/ai\"\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLO(\"/home/annone/ai/models/objseg50e.pt\")\n",
    "model.to(device)\n",
    "print(f\"{device} as Computation Device initiated\")\n",
    "tracker = Sort()\n",
    "\n",
    "# CONSTANTS PER CAMERA\n",
    "width, height = 640,480\n",
    "camera_ip = \"198.78.45.89\"\n",
    "camera_id = 2\n",
    "fps = 30\n",
    "class_list = ['auto', 'bike-rider', 'bolero', 'bus', 'car', 'hatchback', 'jcb', 'motorbike-rider', 'omni', 'pickup',\n",
    "              'scooty-rider', 'scorpio', 'sedan', 'suv', 'swift', 'thar', 'tractor', 'truck', 'van']\n",
    "previous_positions = defaultdict(lambda: {\"x\": 0, \"y\": 0, \"time\": 0})\n",
    "null_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "track_ids_inframe = {}\n",
    "custom_track_ids = {}\n",
    "known_track_ids = []\n",
    "\n",
    "\n",
    "# STREAMING CONSTANTS\n",
    "rtsp_url = 'rtsp://localhost:8554/stream1'  # Update this URL as needed\n",
    "\n",
    "# Construct the FFmpeg command\n",
    "ffmpeg_cmd = [\n",
    "    'ffmpeg',\n",
    "    '-y',  # Overwrite output files without asking\n",
    "    '-f', 'rawvideo',  # Input format\n",
    "    '-pix_fmt', 'bgr24',  # Pixel format\n",
    "    '-s', '640x480',  # Video resolution (adjust as needed)\n",
    "    '-r', '25',  # Frame rate\n",
    "    '-i', '-',  # Input from stdin\n",
    "    '-c:v', 'libx264',  # Video codec\n",
    "    '-preset', 'ultrafast',  # Encoding speed\n",
    "    '-tune', 'zerolatency',  # Tune for low latency\n",
    "    '-f', 'rtsp',  # Output format\n",
    "    rtsp_url  # RTSP output URL\n",
    "]\n",
    "process = subprocess.Popen(ffmpeg_cmd, stdin=subprocess.PIPE)\n",
    "\n",
    "\n",
    "# Function to calculate distance in pixels\n",
    "def calculate_pixel_distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "# Function to calculate speed (assuming pixel distance and time interval)\n",
    "def calculate_speed(pixel_distance, time_interval):\n",
    "    return pixel_distance / time_interval  # Speed in pixels per second\n",
    "\n",
    "# Function to process frames in batches\n",
    "def process_frame_batch(frames):\n",
    "    resized_frames = [cv2.resize(frame, (640 // 32 * 32, 480 // 32 * 32)) for frame in frames]\n",
    "    frames_tensor = torch.from_numpy(np.stack(resized_frames)).permute(0, 3, 1, 2).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_results = model(frames_tensor, device=device)\n",
    "\n",
    "    return batch_results, resized_frames\n",
    "\n",
    "\n",
    "# Function to generate a custom track ID based on YOLO class, confidence, and a unique UUID\n",
    "def generate_custom_track_id(label, confidence):\n",
    "    return f\"{label}_{confidence:.2f}_{uuid.uuid4()}\"\n",
    "\n",
    "# Function to track objects and draw segmentation polygons\n",
    "def track_objects(frames, batch_results, frame_time):\n",
    "    global camera_ip, previous_positions, fps, camera_id, track_ids_inframe, custom_track_ids, known_track_ids\n",
    "\n",
    "    tracked_frames = []\n",
    "    current_track_ids = []  # To keep track of the tracks currently in the frame\n",
    "\n",
    "    for frame, result in zip(frames, batch_results):\n",
    "        detections = []\n",
    "        img_bin = []\n",
    "        labels = []\n",
    "        confs = []\n",
    "\n",
    "        if result.masks:\n",
    "            for mask, box in zip(result.masks.xy, result.boxes):\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                score = box.conf[0].item()\n",
    "                label = model.names[int(box.cls[0])]\n",
    "                detections.append([x1, y1, x2, y2, score,int(box.cls[0])])\n",
    "                labels.append(label)\n",
    "                confs.append(score)\n",
    "                cv2.polylines(frame, [np.array(mask, dtype=np.int32)], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "                cv2.putText(frame, f\"{label} ({score:.2f})\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "\n",
    "        tracks = tracker.update(np.array(detections))\n",
    "\n",
    "        for i, track in enumerate(tracks):\n",
    "            track_id = int(track[4])\n",
    "            x1, y1, x2, y2 = map(int, track[:4])\n",
    "\n",
    "            # If the object is new, generate a custom track ID and store initial data\n",
    "            if track_id not in custom_track_ids:\n",
    "                custom_id = generate_custom_track_id(labels[i], confs[i])\n",
    "                custom_track_ids[track_id] = {\n",
    "                    \"custom_track_id\": custom_id,\n",
    "                    \"camera_id\": camera_id,\n",
    "                    \"camera_ip\": camera_ip,\n",
    "                    \"first_appearance\": frame_time,  # Store first appearance time\n",
    "                    \"last_appearance\": frame_time,   # Initialize last appearance time\n",
    "                    \"dbbox\": [[x1, y1, x2, y2]],\n",
    "                    \"dlabel\": [labels[i]],\n",
    "                    \"dconf\": [confs[i]],\n",
    "                }\n",
    "            else:\n",
    "                # Append the new frame data to the existing object in the dict\n",
    "                custom_track_ids[track_id][\"dbbox\"].append([x1, y1, x2, y2])\n",
    "                custom_track_ids[track_id][\"dlabel\"].append(labels[i])\n",
    "                custom_track_ids[track_id][\"dconf\"].append(confs[i])\n",
    "                custom_track_ids[track_id][\"last_appearance\"] = frame_time  # Update last appearance time\n",
    "\n",
    "            # Add current track ID to the list of track IDs in the current frame\n",
    "            current_track_ids.append(track_id)\n",
    "\n",
    "            # Display the custom track ID on the frame\n",
    "            cv2.putText(frame, f\"ID: {custom_track_ids[track_id]['custom_track_id']}\", (x1, y1 - 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 1)\n",
    "\n",
    "            # Append the current frame to the tracked frames\n",
    "            tracked_frames.append(frame)\n",
    "\n",
    "    # Check for tracks that are no longer in the current frame (left the frame)\n",
    "    tracks_left_frame = set(custom_track_ids.keys()) - set(current_track_ids)\n",
    "\n",
    "    # Insert data into Redis for tracks that left the frame\n",
    "    for track_id in tracks_left_frame:\n",
    "        track_data = custom_track_ids[track_id]\n",
    "        # r.set(track_data['custom_track_id'], str(track_data))  # Insert into Redis as a string or JSON\n",
    "\n",
    "        # Remove the track ID from the custom_track_ids since it left the frame\n",
    "        del custom_track_ids[track_id]\n",
    "\n",
    "    return tracked_frames, list(custom_track_ids.keys())\n",
    "\n",
    "\n",
    "# Function to stream video and process frames in batches\n",
    "def stream_process(camera_id, camera_ip, video_path, batch_size=8):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(\"/home/annone/ai/data/output.mp4\", fourcc, fps, (640,480))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    t1 = time.time()\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Record the current time in seconds for tracking purposes\n",
    "        frame_time = time.time()\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "        if len(frames) >= batch_size:\n",
    "            batch_results, resized_frames = process_frame_batch(frames)\n",
    "\n",
    "            tracked_frames, track_id_list = track_objects(resized_frames, batch_results, frame_time)\n",
    "            for tracked_frame in tracked_frames:\n",
    "                cv2.imshow(\"Tracked Frame\", tracked_frame)\n",
    "                process.stdin.write(tracked_frame.tobytes())\n",
    "                out.write(tracked_frame)\n",
    "            frames = []\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(custom_track_ids)\n",
    "\n",
    "# Example usage\n",
    "video_path = '/home/annone/ai/data/test.mp4'\n",
    "cam_ip = '127.0.0.1'\n",
    "cam_id = \"1\"\n",
    "stream_process(cam_id, cam_ip, video_path, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 1. Mode-Based Approach: Top 3 most frequent labels\n",
    "def top_3_label_mode(dlabel):\n",
    "    label_counts = Counter(dlabel)\n",
    "    top_3_labels = label_counts.most_common(3)\n",
    "    return top_3_labels  # Returns label with its count\n",
    "\n",
    "# 2. Weighted Confidence Sum Approach: Top 3 labels by cumulative confidence\n",
    "def top_3_label_weighted_conf(dlabel, dconf):\n",
    "    weighted_conf_dict = defaultdict(float)\n",
    "    \n",
    "    # Sum the confidences for each label\n",
    "    for label, conf in zip(dlabel, dconf):\n",
    "        weighted_conf_dict[label] += conf\n",
    "    \n",
    "    # Get top 3 labels with highest weighted confidence\n",
    "    top_3_weighted = sorted(weighted_conf_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    return top_3_weighted  # Returns label with its weighted confidence\n",
    "\n",
    "# 3. Highest Confidence Approach: Top 3 labels with the highest individual confidence scores\n",
    "def top_3_label_highest_conf(dlabel, dconf):\n",
    "    # Combine labels and confidences into pairs and sort by confidence\n",
    "    label_conf_pairs = sorted(zip(dlabel, dconf), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top 3 based on confidence\n",
    "    top_3_highest_conf = label_conf_pairs[:3]\n",
    "    \n",
    "    return top_3_highest_conf  # Returns label with its confidence\n",
    "\n",
    "# Example usage\n",
    "dlabel = ['tractor', 'hatchback', 'car', 'car', 'car', 'car', 'car', 'pickup', 'pickup', 'auto', 'auto', 'car', \n",
    "          'car', 'auto', 'auto', 'pickup', 'pickup', 'pickup', 'tractor', 'auto', 'auto', 'tractor', 'auto', \n",
    "          'tractor', 'car', 'car', 'tractor', 'tractor', 'tractor', 'pickup', 'motorbike-rider', 'pickup', 'car', \n",
    "          'tractor', 'car', 'car', 'auto']\n",
    "dconf = [0.275, 0.274, 0.271, 0.333, 0.334, 0.443, 0.444, 0.392, 0.381, 0.429, 0.342, 0.356, \n",
    "         0.349, 0.500, 0.498, 0.387, 0.372, 0.463, 0.341, 0.618, 0.274, 0.350, 0.341, 0.331, \n",
    "         0.300, 0.279, 0.303, 0.391, 0.375, 0.465, 0.360, 0.466, 0.460, 0.434, 0.425, 0.482, 0.408]\n",
    "\n",
    "# Get top 3 selections using different methods\n",
    "top_3_mode = top_3_label_mode(dlabel)\n",
    "top_3_weighted = top_3_label_weighted_conf(dlabel, dconf)\n",
    "top_3_high_conf = top_3_label_highest_conf(dlabel, dconf)\n",
    "\n",
    "# Print results\n",
    "print(\"Top 3 labels (Mode-based):\", top_3_mode)\n",
    "print(\"Top 3 labels (Weighted confidence):\", top_3_weighted)\n",
    "print(\"Top 3 labels (Highest individual confidence):\", top_3_high_conf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
