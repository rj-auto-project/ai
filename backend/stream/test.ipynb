{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  PIL import Image\n",
    "import PIL.Image\n",
    "import redis\n",
    "import PIL\n",
    "import os\n",
    "import cv2\n",
    "# from module import k_mean_color_detection\n",
    "import numpy as np\n",
    "import random\n",
    "from module import  process_raw_d_logs, process_raw_cc_logs, process_cc_logs\n",
    "\n",
    "# Connect to the Redis server\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "def clear_redis_database():\n",
    "    r.flushdb()\n",
    "    print(\"Redis database cleared.\")\n",
    "\n",
    "# Function to store string data with key namespacing\n",
    "def store_string_with_namespace(serial_number, constant_string, value):\n",
    "    full_key = f\"{serial_number}:{constant_string}\"\n",
    "    r.set(full_key, value)\n",
    "    print(f\"Stored {full_key} -> {value}\")\n",
    "\n",
    "# Function to retrieve all data matching the constant part of the key\n",
    "def retrieve_keys(constant_string):\n",
    "    matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "    results = {}\n",
    "    for key in matching_keys:\n",
    "        value = r.get(key)\n",
    "        if value:\n",
    "            decoded_key = key.decode()\n",
    "            decoded_value = value.decode()\n",
    "            # np_arr = np.frombuffer(value, np.uint8)\n",
    "            # Decode the numpy array back to an image\n",
    "            # retrieved_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "            # results[decoded_key] = retrieved_image\n",
    "            results[decoded_key] = decoded_value\n",
    "            # r.delete(decoded_key)\n",
    "            # print(results)\n",
    "            # r.delete(key)  # Delete the key after retrieving the value\n",
    "            # print(f\"Retrieved and deleted {decoded_key} -> {decoded_value}\")\n",
    "    return results\n",
    "\n",
    "def retrieve_image(constant_string):\n",
    "    matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "    results = {}\n",
    "    for key in matching_keys:\n",
    "        value = r.get(key)\n",
    "        if value:\n",
    "            decoded_key = key.decode()\n",
    "            # decoded_value = value.decode()\n",
    "            np_arr = np.frombuffer(value, np.uint8)\n",
    "            # Decode the numpy array back to an image\n",
    "            image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "            # image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "            # image = image.reshape((-1, image.shape[2]))\n",
    "            # cv2.imwrite(f\"/home/annone/ai/backend/stream/d_temp/{decoded_key}.png\", image)\n",
    "            results[decoded_key] = image\n",
    "            # results[decoded_key] = decoded_value\n",
    "            # r.delete(decoded_key)\n",
    "            # print(results)\n",
    "            # r.delete(key)  # Delete the key after retrieving the value\n",
    "            # print(f\"Retrieved and deleted {decoded_key} -> {decoded_value}\")\n",
    "    return results\n",
    "\n",
    "# def process_raw_d_logs(constant_string):\n",
    "#     while True:\n",
    "#         matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "#         # print(matching_keys)\n",
    "#         for key in matching_keys:\n",
    "#             value = r.get(key)\n",
    "#             decode_key = key.decode()\n",
    "#             decode_value = value.decode().split(\"|\")\n",
    "#             track_id = decode_value[5]\n",
    "#             image_path = decode_value[7]\n",
    "#             try:\n",
    "#                 image = cv2.imread(f\"/home/annone/ai-camera/backend/stream/temp/{image_path}\")\n",
    "#                 image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "#                 aa = k_mean_color_detection(image)\n",
    "#                 decode_value[7]  = aa\n",
    "#                 r.set(f\"{track_id}_{random.randint(0,9999)}:d_log\",\"|\".join(decode_value))\n",
    "#                 os.remove(f\"/home/annone/ai-camera/backend/stream/temp/{image_path}\")\n",
    "#                 r.delete(decode_key)\n",
    "#             except:\n",
    "#                 # print(\"excepted\")\n",
    "#                 continue\n",
    "            # show_rgb_colors(aa,image)\n",
    "            # print(decode_value)\n",
    "\n",
    "# def process_d_logs(constant_string):\n",
    "#     conn = Database.get_connection()\n",
    "#     cursor = conn.cursor()\n",
    "#     batch = []\n",
    "#     while True:\n",
    "#         matching_keys = r.keys(f\"*:{constant_string}\")\n",
    "#         for key in matching_keys:\n",
    "#             value = r.get(key)\n",
    "#             decode_key = key.decode()\n",
    "#             decode_value = value.decode().split(\"|\")\n",
    "#             batch.append(tuple(decode_value))\n",
    "#         if len(batch) > 0:\n",
    "#             query = 'INSERT INTO \"DetectionLog\" (\"cameraId\", \"camera_ip\", \"timestamp\", \"boxCoords\", \"detectionClass\", \"trackId\", \"classConfidence\", \"metadata\") VALUES %s;'\n",
    "#             execute_values(cursor, query, batch)\n",
    "#             conn.commit()\n",
    "#             cursor.close()\n",
    "#             conn.close()\n",
    "#             batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_image(\"image\")[\"817_qcYXs5PM_39_1724913805_127_0_0_1_976:image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_keys(\"raw_d_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_redis_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Get all keys from Redis\n",
    "keys = r.keys('*')\n",
    "\n",
    "# Loop through the keys and get the data for each key\n",
    "for i, key in enumerate(keys):\n",
    "    if i ==10:\n",
    "        break\n",
    "    # Decode key to get human-readable format\n",
    "    decoded_key = key.decode('utf-8')\n",
    "    \n",
    "    # Get the value associated with the key\n",
    "    value = r.get(key)\n",
    "    \n",
    "    # Decode the value, if it's stored as a string\n",
    "    decoded_value = value.decode('utf-8') if value else None\n",
    "    \n",
    "    # Print the key-value pair\n",
    "    print(f\"Key: {decoded_key}, Value: {decoded_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundig Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from sort.sort import Sort  # Example of an external tracking library\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLO(\"/home/annone/ai/models/objseg50e.pt\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "tracker = Sort()\n",
    "\n",
    "# Function to process frames in batches\n",
    "def process_frame_batch(frames):\n",
    "    resized_frames = [cv2.resize(frame, (640 // 32 * 32, 480 // 32 * 32)) for frame in frames]\n",
    "\n",
    "    frames_tensor = torch.from_numpy(np.stack(resized_frames)).permute(0, 3, 1, 2).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_results = model(frames_tensor, device=device)\n",
    "\n",
    "    return batch_results, resized_frames\n",
    "\n",
    "def track_objects(frames, batch_results):\n",
    "    tracked_frames = []\n",
    "    \n",
    "    for frame, result in zip(frames, batch_results):\n",
    "        detections = []\n",
    "        \n",
    "        if result.boxes:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                score = box.conf[0].item()\n",
    "                label = model.names[int(box.cls[0])]\n",
    "                \n",
    "                detections.append([x1, y1, x2, y2, score])\n",
    "\n",
    "        tracks = tracker.update(np.array(detections))\n",
    "\n",
    "        for track in tracks:\n",
    "            track_id = int(track[4])\n",
    "            x1, y1, x2, y2 = map(int, track[:4])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        tracked_frames.append(frame)\n",
    "    \n",
    "    return tracked_frames\n",
    "\n",
    "def stream_process(camera_id, camera_ip, video_path, batch_size=8):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    t1 = time.time()\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "        if len(frames) >= batch_size:\n",
    "            batch_results, resized_frames = process_frame_batch(frames)\n",
    "\n",
    "            tracked_frames = track_objects(resized_frames, batch_results)\n",
    "\n",
    "            for tracked_frame in tracked_frames:\n",
    "                cv2.imshow(\"Tracked Frame\", tracked_frame)\n",
    "            frames = []\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "video_path = '/home/annone/ai/data/T-pole wrong way.mp4'\n",
    "cam_ip = '127.0.0.1'\n",
    "cam_id = \"1\"\n",
    "stream_process(cam_id, cam_ip, video_path, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=10\n",
    "count = 0\n",
    "total_parking_violations = 0 \n",
    "wrong_way_violation_count = 0\n",
    "traffic_violation_count = 0  \n",
    "crossed_objects = {}  # Track objects that have crossed lines\n",
    "violated_objects = set()  # Track objects that have already violated\n",
    "static_objects = {}  # To track objects that are stationary\n",
    "stationary_frame_threshold = 200\n",
    "# parking\n",
    "def check_illegal_parking(track_id, cx, cy):\n",
    "    \"\"\"Check if an object is illegally parked based on stationary duration.\"\"\"\n",
    "    global static_objects, total_parking_violations\n",
    "\n",
    "    # Initialize tracking information if not already present\n",
    "    if track_id not in static_objects:\n",
    "        static_objects[track_id] = {\"position\": (cx, cy), \"frames\": 0, \"violated\": False}\n",
    "    else:\n",
    "        last_position = static_objects[track_id][\"position\"]\n",
    "\n",
    "        # Check if the object has remained stationary (within a certain offset)\n",
    "        if abs(cx - last_position[0]) <= offset and abs(cy - last_position[1]) <= offset:\n",
    "            static_objects[track_id][\"frames\"] += 1\n",
    "            # If the object is stationary for more than the threshold and not already marked as a violation\n",
    "            if static_objects[track_id][\"frames\"] > stationary_frame_threshold and not static_objects[track_id][\"violated\"]:\n",
    "                static_objects[track_id][\"violated\"] = True  # Mark the object as a violation\n",
    "                total_parking_violations += 1  # Increase the parking violation count\n",
    "                print(f\"Object {track_id} marked as parking violation. Total Violations: {total_parking_violations}\")\n",
    "        else:\n",
    "            # If the object has moved, reset its tracking information\n",
    "            static_objects[track_id][\"position\"] = (cx, cy)\n",
    "            static_objects[track_id][\"frames\"] = 0\n",
    "            static_objects[track_id][\"violated\"] = False\n",
    "# wrong way\n",
    "def detect_wrong_way_violation(track_id, cx, cy):\n",
    "    global wrong_way_violation_count, violated_objects\n",
    "\n",
    "    # Initialize tracking for the object if not already done\n",
    "    if track_id not in crossed_objects:\n",
    "        crossed_objects[track_id] = {'red': set(), 'green': set()}\n",
    "\n",
    "    # Check if the object crosses any red line\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(red_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                crossed_objects[track_id]['red'].add(i)\n",
    "\n",
    "    # Check if the object crosses any green line\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(green_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                crossed_objects[track_id]['green'].add(i)\n",
    "\n",
    "    # Detect wrong-way violation (crossing green line after red line)\n",
    "    if any(\n",
    "        i in crossed_objects[track_id]['green'] and\n",
    "        i in crossed_objects[track_id]['red'] and\n",
    "        track_id not in violated_objects\n",
    "        for i in crossed_objects[track_id]['green']\n",
    "    ):\n",
    "        # cv2.putText(frame, \"Wrong Way Violation\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        wrong_way_violation_count += 1\n",
    "        violated_objects.add(track_id)\n",
    "# red light violation\n",
    "def detect_traffic_violation(track_id, cx, cy):\n",
    "    \"\"\"Check if an object violates traffic rules by crossing lines.\"\"\"\n",
    "    global traffic_violation_count, crossed_objects, violated_objects\n",
    "\n",
    "    # Initialize tracking for the object if not already done\n",
    "    if track_id not in crossed_objects:\n",
    "        crossed_objects[track_id] = set()\n",
    "\n",
    "    # Check if the object crosses any red line\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(red_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                crossed_objects[track_id].add(f\"red_{i}\")\n",
    "\n",
    "    # Check if the object crosses any green line after crossing a red line (indicating a violation)\n",
    "    for i, ((x_start, y_start), (x_end, y_end)) in enumerate(green_lines):\n",
    "        if min(y_start, y_end) - offset <= cy <= max(y_start, y_end) + offset:\n",
    "            if min(x_start, x_end) <= cx <= max(x_start, x_end):\n",
    "                if any(f\"red_{j}\" in crossed_objects[track_id] for j in range(len(red_lines))) and track_id not in violated_objects:\n",
    "                    # cv2.putText(frame, \"Traffic Violation\", (cx, cy - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "                    traffic_violation_count += 1\n",
    "                    violated_objects.add(track_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from sort.sort import Sort\n",
    "import time\n",
    "# from module import generate_custom_string\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import redis\n",
    "import uuid\n",
    "import subprocess\n",
    "\n",
    "# Initialize device and model\n",
    "PARENT_DIR = \"/home/annone/ai\"\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLO(\"/home/annone/ai/models/objseg50e.pt\")\n",
    "model.to(device)\n",
    "print(f\"{device} as Computation Device initiated\")\n",
    "tracker = Sort()\n",
    "\n",
    "# CONSTANTS PER CAMERA\n",
    "width, height = 640,480\n",
    "camera_ip = \"198.78.45.89\"\n",
    "camera_id = 2\n",
    "fps = 30\n",
    "class_list = ['auto', 'bike-rider', 'bolero', 'bus', 'car', 'hatchback', 'jcb', 'motorbike-rider', 'omni', 'pickup',\n",
    "              'scooty-rider', 'scorpio', 'sedan', 'suv', 'swift', 'thar', 'tractor', 'truck', 'van']\n",
    "previous_positions = defaultdict(lambda: {\"x\": 0, \"y\": 0, \"time\": 0})\n",
    "null_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "track_ids_inframe = {}\n",
    "custom_track_ids = {}\n",
    "known_track_ids = []\n",
    "\n",
    "\n",
    "# STREAMING CONSTANTS\n",
    "rtsp_url = 'rtsp://localhost:8554/stream1'  # Update this URL as needed\n",
    "\n",
    "# Construct the FFmpeg command\n",
    "ffmpeg_cmd = [\n",
    "    'ffmpeg',\n",
    "    '-y',  # Overwrite output files without asking\n",
    "    '-f', 'rawvideo',  # Input format\n",
    "    '-pix_fmt', 'bgr24',  # Pixel format\n",
    "    '-s', '640x480',  # Video resolution (adjust as needed)\n",
    "    '-r', '25',  # Frame rate\n",
    "    '-i', '-',  # Input from stdin\n",
    "    '-c:v', 'libx264',  # Video codec\n",
    "    '-preset', 'ultrafast',  # Encoding speed\n",
    "    '-tune', 'zerolatency',  # Tune for low latency\n",
    "    '-f', 'rtsp',  # Output format\n",
    "    rtsp_url  # RTSP output URL\n",
    "]\n",
    "process = subprocess.Popen(ffmpeg_cmd, stdin=subprocess.PIPE)\n",
    "\n",
    "\n",
    "# Function to calculate distance in pixels\n",
    "def calculate_pixel_distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "# Function to calculate speed (assuming pixel distance and time interval)\n",
    "def calculate_speed(pixel_distance, time_interval):\n",
    "    return pixel_distance / time_interval  # Speed in pixels per second\n",
    "\n",
    "# Function to process frames in batches\n",
    "def process_frame_batch(frames):\n",
    "    resized_frames = [cv2.resize(frame, (640 // 32 * 32, 480 // 32 * 32)) for frame in frames]\n",
    "    frames_tensor = torch.from_numpy(np.stack(resized_frames)).permute(0, 3, 1, 2).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_results = model(frames_tensor, device=device)\n",
    "\n",
    "    return batch_results, resized_frames\n",
    "\n",
    "\n",
    "# Function to generate a custom track ID based on YOLO class, confidence, and a unique UUID\n",
    "def generate_custom_track_id(label, confidence):\n",
    "    return f\"{label}_{confidence:.2f}_{uuid.uuid4()}\"\n",
    "\n",
    "# Function to track objects and draw segmentation polygons\n",
    "def track_objects(frames, batch_results, frame_time):\n",
    "    global camera_ip, previous_positions, fps, camera_id, track_ids_inframe, custom_track_ids, known_track_ids\n",
    "\n",
    "    tracked_frames = []\n",
    "    current_track_ids = []  # To keep track of the tracks currently in the frame\n",
    "\n",
    "    for frame, result in zip(frames, batch_results):\n",
    "        detections = []\n",
    "        img_bin = []\n",
    "        labels = []\n",
    "        confs = []\n",
    "\n",
    "        if result.masks:\n",
    "            for mask, box in zip(result.masks.xy, result.boxes):\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                score = box.conf[0].item()\n",
    "                label = model.names[int(box.cls[0])]\n",
    "                detections.append([x1, y1, x2, y2, score,int(box.cls[0])])\n",
    "                labels.append(label)\n",
    "                confs.append(score)\n",
    "                cv2.polylines(frame, [np.array(mask, dtype=np.int32)], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "                cv2.putText(frame, f\"{label} ({score:.2f})\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "\n",
    "        tracks = tracker.update(np.array(detections))\n",
    "\n",
    "        for i, track in enumerate(tracks):\n",
    "            track_id = int(track[4])\n",
    "            x1, y1, x2, y2 = map(int, track[:4])\n",
    "\n",
    "            # If the object is new, generate a custom track ID and store initial data\n",
    "            if track_id not in custom_track_ids:\n",
    "                custom_id = generate_custom_track_id(labels[i], confs[i])\n",
    "                custom_track_ids[track_id] = {\n",
    "                    \"custom_track_id\": custom_id,\n",
    "                    \"camera_id\": camera_id,\n",
    "                    \"camera_ip\": camera_ip,\n",
    "                    \"first_appearance\": frame_time,  # Store first appearance time\n",
    "                    \"last_appearance\": frame_time,   # Initialize last appearance time\n",
    "                    \"dbbox\": [[x1, y1, x2, y2]],\n",
    "                    \"dlabel\": [labels[i]],\n",
    "                    \"dconf\": [confs[i]],\n",
    "                }\n",
    "            else:\n",
    "                # Append the new frame data to the existing object in the dict\n",
    "                custom_track_ids[track_id][\"dbbox\"].append([x1, y1, x2, y2])\n",
    "                custom_track_ids[track_id][\"dlabel\"].append(labels[i])\n",
    "                custom_track_ids[track_id][\"dconf\"].append(confs[i])\n",
    "                custom_track_ids[track_id][\"last_appearance\"] = frame_time  # Update last appearance time\n",
    "\n",
    "            # Add current track ID to the list of track IDs in the current frame\n",
    "            current_track_ids.append(track_id)\n",
    "\n",
    "            # Display the custom track ID on the frame\n",
    "            cv2.putText(frame, f\"ID: {custom_track_ids[track_id]['custom_track_id']}\", (x1, y1 - 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 1)\n",
    "\n",
    "            # Append the current frame to the tracked frames\n",
    "            tracked_frames.append(frame)\n",
    "\n",
    "    # Check for tracks that are no longer in the current frame (left the frame)\n",
    "    tracks_left_frame = set(custom_track_ids.keys()) - set(current_track_ids)\n",
    "\n",
    "    # Insert data into Redis for tracks that left the frame\n",
    "    for track_id in tracks_left_frame:\n",
    "        track_data = custom_track_ids[track_id]\n",
    "        # r.set(track_data['custom_track_id'], str(track_data))  # Insert into Redis as a string or JSON\n",
    "\n",
    "        # Remove the track ID from the custom_track_ids since it left the frame\n",
    "        del custom_track_ids[track_id]\n",
    "\n",
    "    return tracked_frames, list(custom_track_ids.keys())\n",
    "\n",
    "\n",
    "# Function to stream video and process frames in batches\n",
    "def stream_process(camera_id, camera_ip, video_path, batch_size=8):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(\"/home/annone/ai/data/output.mp4\", fourcc, fps, (640,480))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    t1 = time.time()\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Record the current time in seconds for tracking purposes\n",
    "        frame_time = time.time()\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "        if len(frames) >= batch_size:\n",
    "            batch_results, resized_frames = process_frame_batch(frames)\n",
    "\n",
    "            tracked_frames, track_id_list = track_objects(resized_frames, batch_results, frame_time)\n",
    "            for tracked_frame in tracked_frames:\n",
    "                cv2.imshow(\"Tracked Frame\", tracked_frame)\n",
    "                process.stdin.write(tracked_frame.tobytes())\n",
    "                out.write(tracked_frame)\n",
    "            frames = []\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(custom_track_ids)\n",
    "\n",
    "# Example usage\n",
    "video_path = '/home/annone/ai/data/test.mp4'\n",
    "cam_ip = '127.0.0.1'\n",
    "cam_id = \"1\"\n",
    "stream_process(cam_id, cam_ip, video_path, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 1. Mode-Based Approach: Top 3 most frequent labels\n",
    "def top_3_label_mode(dlabel):\n",
    "    label_counts = Counter(dlabel)\n",
    "    top_3_labels = label_counts.most_common(3)\n",
    "    return top_3_labels  # Returns label with its count\n",
    "\n",
    "# 2. Weighted Confidence Sum Approach: Top 3 labels by cumulative confidence\n",
    "def top_3_label_weighted_conf(dlabel, dconf):\n",
    "    weighted_conf_dict = defaultdict(float)\n",
    "    \n",
    "    # Sum the confidences for each label\n",
    "    for label, conf in zip(dlabel, dconf):\n",
    "        weighted_conf_dict[label] += conf\n",
    "    \n",
    "    # Get top 3 labels with highest weighted confidence\n",
    "    top_3_weighted = sorted(weighted_conf_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    return top_3_weighted  # Returns label with its weighted confidence\n",
    "\n",
    "# 3. Highest Confidence Approach: Top 3 labels with the highest individual confidence scores\n",
    "def top_3_label_highest_conf(dlabel, dconf):\n",
    "    # Combine labels and confidences into pairs and sort by confidence\n",
    "    label_conf_pairs = sorted(zip(dlabel, dconf), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top 3 based on confidence\n",
    "    top_3_highest_conf = label_conf_pairs[:3]\n",
    "    \n",
    "    return top_3_highest_conf  # Returns label with its confidence\n",
    "\n",
    "# Example usage\n",
    "dlabel = ['tractor', 'hatchback', 'car', 'car', 'car', 'car', 'car', 'pickup', 'pickup', 'auto', 'auto', 'car', \n",
    "          'car', 'auto', 'auto', 'pickup', 'pickup', 'pickup', 'tractor', 'auto', 'auto', 'tractor', 'auto', \n",
    "          'tractor', 'car', 'car', 'tractor', 'tractor', 'tractor', 'pickup', 'motorbike-rider', 'pickup', 'car', \n",
    "          'tractor', 'car', 'car', 'auto']\n",
    "dconf = [0.275, 0.274, 0.271, 0.333, 0.334, 0.443, 0.444, 0.392, 0.381, 0.429, 0.342, 0.356, \n",
    "         0.349, 0.500, 0.498, 0.387, 0.372, 0.463, 0.341, 0.618, 0.274, 0.350, 0.341, 0.331, \n",
    "         0.300, 0.279, 0.303, 0.391, 0.375, 0.465, 0.360, 0.466, 0.460, 0.434, 0.425, 0.482, 0.408]\n",
    "\n",
    "# Get top 3 selections using different methods\n",
    "top_3_mode = top_3_label_mode(dlabel)\n",
    "top_3_weighted = top_3_label_weighted_conf(dlabel, dconf)\n",
    "top_3_high_conf = top_3_label_highest_conf(dlabel, dconf)\n",
    "\n",
    "# Print results\n",
    "print(\"Top 3 labels (Mode-based):\", top_3_mode)\n",
    "print(\"Top 3 labels (Weighted confidence):\", top_3_weighted)\n",
    "print(\"Top 3 labels (Highest individual confidence):\", top_3_high_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  6 20:36:34 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8             10W /   55W |      15MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2351      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "80 ======\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
      "0: 256x256 (no detections), 3.8ms\n",
      "1: 256x256 (no detections), 3.8ms\n",
      "2: 256x256 (no detections), 3.8ms\n",
      "3: 256x256 (no detections), 3.8ms\n",
      "4: 256x256 (no detections), 3.8ms\n",
      "5: 256x256 (no detections), 3.8ms\n",
      "6: 256x256 (no detections), 3.8ms\n",
      "7: 256x256 (no detections), 3.8ms\n",
      "8: 256x256 (no detections), 3.8ms\n",
      "9: 256x256 (no detections), 3.8ms\n",
      "10: 256x256 (no detections), 3.8ms\n",
      "11: 256x256 (no detections), 3.8ms\n",
      "12: 256x256 (no detections), 3.8ms\n",
      "13: 256x256 (no detections), 3.8ms\n",
      "14: 256x256 (no detections), 3.8ms\n",
      "15: 256x256 1 person, 3.8ms\n",
      "16: 256x256 (no detections), 3.8ms\n",
      "17: 256x256 (no detections), 3.8ms\n",
      "18: 256x256 (no detections), 3.8ms\n",
      "19: 256x256 1 person, 3.8ms\n",
      "20: 256x256 (no detections), 3.8ms\n",
      "21: 256x256 (no detections), 3.8ms\n",
      "22: 256x256 1 person, 3.8ms\n",
      "23: 256x256 (no detections), 3.8ms\n",
      "24: 256x256 (no detections), 3.8ms\n",
      "25: 256x256 (no detections), 3.8ms\n",
      "26: 256x256 (no detections), 3.8ms\n",
      "27: 256x256 (no detections), 3.8ms\n",
      "28: 256x256 (no detections), 3.8ms\n",
      "29: 256x256 (no detections), 3.8ms\n",
      "30: 256x256 (no detections), 3.8ms\n",
      "31: 256x256 (no detections), 3.8ms\n",
      "32: 256x256 (no detections), 3.8ms\n",
      "33: 256x256 (no detections), 3.8ms\n",
      "34: 256x256 (no detections), 3.8ms\n",
      "35: 256x256 (no detections), 3.8ms\n",
      "36: 256x256 (no detections), 3.8ms\n",
      "37: 256x256 (no detections), 3.8ms\n",
      "38: 256x256 (no detections), 3.8ms\n",
      "39: 256x256 (no detections), 3.8ms\n",
      "40: 256x256 (no detections), 3.8ms\n",
      "41: 256x256 (no detections), 3.8ms\n",
      "42: 256x256 (no detections), 3.8ms\n",
      "43: 256x256 (no detections), 3.8ms\n",
      "44: 256x256 (no detections), 3.8ms\n",
      "45: 256x256 (no detections), 3.8ms\n",
      "46: 256x256 (no detections), 3.8ms\n",
      "47: 256x256 (no detections), 3.8ms\n",
      "48: 256x256 (no detections), 3.8ms\n",
      "49: 256x256 (no detections), 3.8ms\n",
      "50: 256x256 (no detections), 3.8ms\n",
      "51: 256x256 (no detections), 3.8ms\n",
      "52: 256x256 (no detections), 3.8ms\n",
      "53: 256x256 (no detections), 3.8ms\n",
      "54: 256x256 (no detections), 3.8ms\n",
      "55: 256x256 (no detections), 3.8ms\n",
      "56: 256x256 (no detections), 3.8ms\n",
      "57: 256x256 (no detections), 3.8ms\n",
      "58: 256x256 (no detections), 3.8ms\n",
      "59: 256x256 1 person, 3.8ms\n",
      "60: 256x256 (no detections), 3.8ms\n",
      "61: 256x256 (no detections), 3.8ms\n",
      "62: 256x256 (no detections), 3.8ms\n",
      "63: 256x256 (no detections), 3.8ms\n",
      "64: 256x256 (no detections), 3.8ms\n",
      "65: 256x256 1 person, 3.8ms\n",
      "66: 256x256 1 person, 3.8ms\n",
      "67: 256x256 (no detections), 3.8ms\n",
      "68: 256x256 (no detections), 3.8ms\n",
      "69: 256x256 (no detections), 3.8ms\n",
      "70: 256x256 (no detections), 3.8ms\n",
      "71: 256x256 (no detections), 3.8ms\n",
      "72: 256x256 (no detections), 3.8ms\n",
      "73: 256x256 (no detections), 3.8ms\n",
      "74: 256x256 (no detections), 3.8ms\n",
      "75: 256x256 (no detections), 3.8ms\n",
      "76: 256x256 (no detections), 3.8ms\n",
      "77: 256x256 (no detections), 3.8ms\n",
      "78: 256x256 (no detections), 3.8ms\n",
      "79: 256x256 (no detections), 3.8ms\n",
      "Speed: 0.2ms preprocess, 3.8ms inference, 0.9ms postprocess per image at shape (1, 3, 256, 256)\n",
      "80\n",
      "0.840184211730957\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from function import ViolationDetector  # Import your class from the first file\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "var = ViolationDetector()\n",
    "img = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "for j,i in enumerate(os.listdir(\"/home/annone/ai/images/urination\")):\n",
    "    if j == 80:\n",
    "        break\n",
    "    if i.endswith(\".png\"):  # Check if the file is a JPG image\n",
    "        img_path = os.path.join(\"/home/annone/ai/images/urination\", i)  # Create the full path\n",
    "        img.append(cv2.imread(img_path))\n",
    "\n",
    "model_path = '/home/annone/ai/models/pee_spit.pth'\n",
    "t1 = time.time()\n",
    "results = var.process_image(img,model_path , \"uri_000\", \"000000\", \"00000\")\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
